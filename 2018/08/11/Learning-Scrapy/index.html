<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>基于Scrapy爬虫开发 | NJUCS-CVICSE</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-121783408-3','auto');ga('send','pageview');
</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">基于Scrapy爬虫开发</h1><a id="logo" href="/.">NJUCS-CVICSE</a><p class="description">联合实验室工作日志</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">基于Scrapy爬虫开发</h1><div class="post-meta">Aug 11, 2018</div><div class="post-content"><h2 id="Scrapy简介"><a href="#Scrapy简介" class="headerlink" title="Scrapy简介"></a>Scrapy简介</h2><p>Scrapy是由Python开发的一个快速,高层次的web抓取框架，一般用于抓取web站点，并从页面中提取结构化的数据。<br>用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。<br>Scrapy 使用了 Twisted’twɪstɪd异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。</p>
<h2 id="Scrapy安装（for-Mac）"><a href="#Scrapy安装（for-Mac）" class="headerlink" title="Scrapy安装（for Mac）"></a>Scrapy安装（for Mac）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>
<p>安装过程中可能会比较慢，或是在Collecting Twisted的时候卡住，可以考虑先安装Twisted或是配个国内的镜像源加速。</p>
<h2 id="如何开始Scrapy爬虫"><a href="#如何开始Scrapy爬虫" class="headerlink" title="如何开始Scrapy爬虫"></a>如何开始Scrapy爬虫</h2><h3 id="创建一个项目"><a href="#创建一个项目" class="headerlink" title="创建一个项目"></a>创建一个项目</h3><p>我们用命令行的方式创建一个scrapy项目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject tutorial</span><br></pre></td></tr></table></figure>
<p>然后进入项目目录，根据内置模版创建一个爬取中国商务部网站的爬虫，这个爬虫名字叫mofocm ，爬虫允许在mofcom.gov.cn 域名下爬取</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd tutorial</span><br><span class="line">scrapy genspider mofcom mofcom.gov.cn</span><br></pre></td></tr></table></figure>
<p>此时在spiders目录下会生成mofcom.py文件，我们主要是在这个模块下对得到的网页内容进行解析，这个模块下定义了parse方法，默认对得到的Response对象提取出我们需要的结构化的数据。</p>
<p>在mofcom.py中我们可以看到，其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的parse方法。我们创建的Spider中，继承 scrapy.Spider 类，其包含了三个属性：name,start_urls,parse()方法</p>
<ul>
<li>name： 用来标识spider 的名字</li>
<li>start_urls: 包含了Spider在启动时进行爬取的url列表</li>
<li>parse()方法：是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据以及生成需要进一步处理的URL的 Request 对象</li>
</ul>
<p>下面给出一个示例代码，保存在在mofcom.py中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MofcomSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"mofcom"</span></span><br><span class="line">    allowed_domains = [<span class="string">"mofcom.gov.cn"</span>]</span><br><span class="line">    start_urls = [    <span class="string">"http://www.mofcom.gov.cn/article/ae/ai"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">          <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//li/a'</span>):</span><br><span class="line">            item = TutorialItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'./@title'</span>).extract()</span><br><span class="line">            site = sel.xpath(<span class="string">'./@href'</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> site.startswith(<span class="string">'http'</span>):</span><br><span class="line">                url = <span class="string">"http://www.mofcom.gov.cn"</span> + site</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                url = site</span><br><span class="line">            item[<span class="string">'link'</span>] = url</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>这会对从‘<a href="http://www.mofcom.gov.cn/article/ae/ai‘" target="_blank" rel="noopener">http://www.mofcom.gov.cn/article/ae/ai‘</a> 得到的response进行解析。scrapy在运行爬虫的时候，首先会根据start_urls 中的URL生成Request，Request对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。</p>
<h3 id="定义item"><a href="#定义item" class="headerlink" title="定义item"></a>定义item</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TutorialItem</span><span class="params">(Object)</span>:</span></span><br><span class="line">     title = scrapy.Field()</span><br><span class="line">     link = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>为了从得到的页面中提取出结构化的数据，我们必须定义相应的item，item是Python字典Dict类型。(字段即是我们之前用Field赋值的属性)</p>
<p>一般来说，Spider将会将爬取到的数据以item对象的形式返回。</p>
<h3 id="自定义的pipeline的实现"><a href="#自定义的pipeline的实现" class="headerlink" title="自定义的pipeline的实现"></a>自定义的pipeline的实现</h3><p>在提取出相应的数据之后，我们可能还要对得到的数据进行存储，去重或是过滤等操作，此时就需要借助pipeline组件。</p>
<p>例如我们需要将我们得到的数据输出为json文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWithEncodingPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = codecs.open(<span class="string">'data_utf8.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(OrderedDict(item), ensure_ascii=<span class="keyword">False</span>, sort_keys=<span class="keyword">False</span>) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="comment">#print line</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure></p>
<p>这会将你提取的数据输出成data_utf8.json文件。</p>
<p>每个item pipiline组件必须实现process_item(item, spider)，这个方法定义对提取出来的item进行的操作，该方法必须返回一个 Item (或任何继承类)对象， 或是抛出 DropItem 异常，而之后被丢弃的item将不会被之后的pipeline组件所处理。</p>
<p>另外还可以实现其他方法<br>open_spider(spider)，当spider被开启时，这个方法被调用；<br>close_spider(spider)，当spider被关闭时，这个方法被调用。</p>
<p>为了启用Pipeline组件，你还要在setting文件中将它的类添加到 ITEM_PIPELINES 配置<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'tutorial.pipelines.JsonWithEncodingPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中300 并没有什么具体的含义，只是为了在多个pipeline是用来进行排序,判断那个pipelines先运行</p>
<h3 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h3><p>setting文件中将ROBOTSTXT_OBEY 默认为True，就是要遵守robots.txt 的规则， 现在要改为为false，不然无法爬取。<br>如果一切都准确无误的话，就可以开始运行爬虫了，在项目跟目录执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl mofcom</span><br></pre></td></tr></table></figure></p>
<p>此时爬虫变开始爬取网站数据</p>
<h3 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h3><p>当我们抓取网页时，做的最常见的任务是从HTML源码中提取数据。Scrapy使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors，从网页提取数据，它们通过特定的 XPath 或者 CSS 表达式来“选择” HTML文件中的某个部分。<br>例如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.xpath(<span class="string">'//li/a'</span>)</span><br></pre></td></tr></table></figure></p>
<p>这会提取出HTML文档li节点内所有的a元素<br>Selector有几个个基本的方法</p>
<ul>
<li>XPath() 返回XPath表达式所对应的节点的selector list列表</li>
<li>css() 返回css表达式所对应的节点的selector list列表</li>
<li>extract()  序列化该节点为unicode字符串并返回list</li>
<li>extract_first() 序列化该节点为unicode字符串并返回list的第一个字符串</li>
<li>re() 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表</li>
</ul>
<h3 id="Scrapy-shell"><a href="#Scrapy-shell" class="headerlink" title="Scrapy shell"></a>Scrapy shell</h3><p>Scrapy终端是一个交互终端，供您在未启动spider的情况下尝试及调试您的爬取代码，测试提取数据的代码，在shell中你可以用它来测试你的selector是否能够正确地提取出相关的数据。我们可以在scrapy shell 中测试我们的selector是否能够提取出我们要的数据，而不用之前启动爬虫调试，这样的话大大减少了我们的工作量</p>
<p>举个例子，还是那个商务网站：<br>现在先在命令行里面敲<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell http://www.mofcom.gov.cn/article/ae/ai</span><br></pre></td></tr></table></figure></p>
<p>这样进入shell,当shell载入后，您将得到一个包含response数据的本地 response 变量。通过查看源码，我们可以发现当前页面的新闻信息都在 //li/a 中。我们可以通过以下代码提取出在 li节点内的所有a元素<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sel = response.xpath(<span class="string">'//li/a'</span>)</span><br></pre></td></tr></table></figure></p>
<p>对应的每一条新闻的标题则是<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sel.xpath(<span class="string">'./@title'</span>).extract_first()</span><br></pre></td></tr></table></figure></p>
<p>对应的每一条新闻的链接则是<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sel.xpath(<span class="string">'./@href'</span>).extract_first()</span><br></pre></td></tr></table></figure></p>
<p>这样我们便可以Scrapy shell中用Selector得到的结果用来调试我们的代码。</p>
<p>有时如果想在spider的某个位置中查看被处理的response， 以确认自己期望的response到达特定位置。这个可以通过 scrapy.shell.inspect_response 函数来实现。只需要在parse某个位置加上<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.shell <span class="keyword">import</span> inspect_response</span><br><span class="line">inspect_response(response, self)</span><br></pre></td></tr></table></figure></p>
<p>spider运行到该处的时候，就会进入shell进行调试，如果你想查看response或是selector是否正确，可以在shell中查看使用selector提取出的数据。点击Ctrl-D进行退出，爬虫会继续执行。</p>
<h3 id="scrapy-Spider"><a href="#scrapy-Spider" class="headerlink" title="scrapy.Spider"></a>scrapy.Spider</h3><p>scrapy.Spider是最简单的spider。每个其他的spider必须继承自该类(包括Scrapy自带的其他spider以及您自己编写的spider),其主要作用就是有个默认的实现方法 start_requests()，该方法会读取start_urls中的URL生成Request，然后爬虫会根据返回的结构response调用回调函数parse。<br>我们的spiders并没有实现start_requests()，这是调用了默认的start_requests(),这个会默认使用回调函数parse。如果你不想使用parse，或是自己还实现了其他方法例如parse_xxx,你也可以在start_request中去更改，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.Request(URL, callback=self.parse_XXX)</span><br></pre></td></tr></table></figure></p>
<p>其实在parse解析函数中出了可以提取item，也可以产生request，这样产生的Request会经过scrapy引擎的调度继续去请求获取网页数据，得到的数据又会被回调函数解析，像上面的代码产生得到的数据会被parse_XXX 解析。</p>
<p>我们可以以这样的方式进入到下一层网页爬取更详细的数据，例如如果我想爬取当前的URL得到的所有链接所指向的新闻内容，我们可以在parse中加入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.Request(item[<span class="string">'link'</span>], callback=self.parse_more,meta=&#123;<span class="string">'item'</span>: item&#125;)</span><br></pre></td></tr></table></figure></p>
<p>这个会产生一个Request请求每一条新闻的内容，在TutorialItem添加desc属性，另外我们还需要实现一下parse_more</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_more</span><span class="params">(self,response)</span>:</span></span><br><span class="line">            item = response.meta[<span class="string">'item'</span>]</span><br><span class="line">            <span class="comment"># populate more `item` fields</span></span><br><span class="line">            sel = response.xpath(<span class="string">'//div[@id="zoom"]'</span>).xpath(<span class="string">'string(.)'</span>).extract_first().strip()</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>那么爬虫会爬取进一步爬取新闻的内容信息。</p>
<h3 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h3><p>CrawlSpider爬取一般网站常用的spider。其定义了一些规则(rule)来提供跟进link的方便的机制，这个爬虫能够根据规则对一些符合规则对链接进行跟进。它也是从scrapy.Spider 继承过来的，所以除了scrapy.Spider 的属性外，其提供了一个新的属性:rules。这是一个包含一个(或多个) Rule 对象的集合(list）。<br>Rule对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">spiders</span>.<span class="title">Rule</span><span class="params">(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)</span></span></span><br></pre></td></tr></table></figure></p>
<p>Rule对象中包含一个LinkExtractor对象,其定义了如何从爬取到的页面提取链接。它定义了allow和deny属性，它会对符合要求的链接跟进，并在提取出符合rules中Rule的链接，进行相应的处理，而follow则是决定是否跟进链接。rules中的规则优先级是总是前面的Rule优先。</p>
<p>rules<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">       Rule(LinkExtractor(</span><br><span class="line">           allow=(<span class="string">r'article/ae/[a-z]*?/2018[0-9]*?/[0-9]*?\.shtml'</span>, ),</span><br><span class="line">           deny=(<span class="string">r'http://www.mofcom.gov.cn/article/ae/slfw/2018[0-9]*?/[0-9]*?\.shtml'</span>, 		  <span class="string">r'http://www.mofcom.gov.cn/article/ae/ztfbh/2018[0-9]*?/[0-9]*?\.shtml'</span>)),</span><br><span class="line">           callback=<span class="string">'parse_content'</span>),</span><br><span class="line"></span><br><span class="line">       Rule(LinkExtractor(allow=(<span class="string">'/article/ae/ai'</span>, <span class="string">'/article/ae/ag'</span>)),</span><br><span class="line">            follow=<span class="keyword">True</span>),</span><br><span class="line">   )</span><br></pre></td></tr></table></figure></p>
<p>以上定义的两个Rule对象，会先考虑在原网页中提取出符合正则表达式的链接，像第一个Rule，提取出相关链接后，会调用parse_content回调函数，而第二个Rule会继续跟进提取出来的链接。</p>
<p>需要注意的是：当编写爬虫规则时，请避免使用 parse 作为回调函数。 由于 CrawlSpider 使用 parse 方法来实现其逻辑，如果覆盖了 parse 方法，CrawlSpider 将会运行失败</p>
<p>如果要了解多一些东西的话还是需要看scrapy文档 <a href="https://doc.scrapy.org/en/latest/" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/</a></p>
</div><div class="tags"><a href="/tags/学习笔记/">学习笔记</a></div><div class="post-nav"><a class="next" href="/2018/07/27/Product-classification/">网络中的节点分类</a></div><div id="container"></div><link rel="stylesheet" href="/css/default.css?v=0.0.0"><script src="/js/gitment.browser.js?v=0.0.0"></script><script>var gitment = new Gitment({
  owner: 'caochun',
  repo: 'comments-cvicse',
  oauth: {
    client_id: '502baa5b22416eebf178',
    client_secret: 'dede4f9ce1c22f8333524381b1d23895da54e660',
  },
})
gitment.render('container')
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://cvicse.njuics.cn"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/学习笔记/" style="font-size: 15px;">学习笔记</a> <a href="/tags/项目纪要/" style="font-size: 15px;">项目纪要</a> <a href="/tags/个人小结/" style="font-size: 15px;">个人小结</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/08/11/Learning-Scrapy/">基于Scrapy爬虫开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/27/Product-classification/">网络中的节点分类</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/07/Graph-Mining/">基于机器学习的质监预测技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/26/Data-Explaination/">质监数据说明</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/24/week-summary-suchuan/">南京大学学习周报-苏川</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/24/week-summary-pcq/">图数据库Neo4j学习总结-彭长青</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/23/First-Using-GitAndGithub/">Git与GitHub入门教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/21/Graph-based-Enterprise-Portrait/">基于图数据库的企业画像</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/20/Data-Processing-for-Shanghai-Stock-Exchange/">上证所交易数据处理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/10/Real-Time-Tolling/">高速计费实时数据处理框架</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">NJUCS-CVICSE.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>